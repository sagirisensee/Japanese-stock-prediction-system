import requests
from bs4 import BeautifulSoup
import json
import time
import os
import re
from datetime import datetime, timedelta
import sys

# åŠ è½½é…ç½®
from config import Config

# éªŒè¯é…ç½®
if not Config.validate():
    print("\nâŒ é…ç½®ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
    sys.exit(1)

# --- ä½¿ç”¨é…ç½® ---
QWEN_API_KEY = Config.QWEN_API_KEY
GEMINI_API_KEY = Config.GEMINI_API_KEY
MODEL_ID = Config.GEMINI_MODEL_ID

def get_save_dir():
    folder = f"report_{datetime.now().strftime('%Y%m%d')}"
    if not os.path.exists(folder):
        os.makedirs(folder)
    return folder

def is_weekend():
    """åˆ¤æ–­ä»Šå¤©æ˜¯å¦æ˜¯å‘¨äº”/å‘¨å…­/å‘¨æ—¥"""
    return datetime.now().weekday() in [4, 5, 6]  # 4=å‘¨äº”, 5=å‘¨å…­, 6=å‘¨æ—¥

def get_next_trading_day(from_date=None):
    """è·å–ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥"""
    if from_date is None:
        today = datetime.now()
    else:
        today = from_date

    next_day = today + timedelta(days=1)
    # è·³è¿‡å‘¨æœ«
    while next_day.weekday() >= 5:
        next_day += timedelta(days=1)
    return next_day.strftime('%Y-%m-%d')

def get_weekend_cache_file():
    """è·å–å‘¨æœ«ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    cache_dir = "./weekend_cache"
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)

    # æ‰¾åˆ°æœ¬å‘¨äº”çš„æ—¥æœŸä½œä¸ºæ ‡è¯†
    today = datetime.now()
    # è®¡ç®—è·ç¦»ä¸Šä¸€ä¸ªæˆ–å½“å‰å‘¨äº”çš„å¤©æ•°
    days_since_friday = (today.weekday() - 4) % 7
    this_friday = today - timedelta(days=days_since_friday)

    return f"{cache_dir}/weekend_{this_friday.strftime('%Y%m%d')}.json"

# 1. æŠ“å–æ¨¡å—
def fetch_80_titles():
    now = datetime.now()
    target_dt = now - timedelta(days=1) if now.hour < 3 else now
    target_date_short = target_dt.strftime('%-m/%-d')

    print(f"ğŸ¯ æ­£åœ¨æ£€ç´¢æ—¥æœŸä¸º {target_date_short} çš„æ–°é—»æ ‡é¢˜...")

    url = "https://finance.yahoo.co.jp/news/bus_all"
    headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"}

    titles_pool = []
    for page in range(1, 6):
        try:
            res = requests.get(f"{url}?page={page}", headers=headers, timeout=30)
            soup = BeautifulSoup(res.text, 'html.parser')
            news_items = soup.select('a[href*="/news/detail/"]')
            for a in news_items:
                title = a.get_text(strip=True)
                parent = a.find_parent()
                time_text = parent.get_text() if parent else ""

                if (target_date_short in time_text) or (":" in time_text and "/" not in time_text):
                    if not any(t['title'] == title for t in titles_pool):
                        # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                        href = a['href']
                        if href.startswith('/'):
                            href = f"https://finance.yahoo.co.jp{href}"
                        titles_pool.append({"title": title, "url": href})
            if len(titles_pool) >= 80: break
            time.sleep(1)
        except: break
    return titles_pool[:80]

# 2. Gemini åˆç­›
def gemini_stage1_filter(titles_list, target_count=20):
    print(f"âš¡ï¸ Gemini 2.5 Flash æ­£åœ¨é«˜é€Ÿåˆç­›...")
    url = f"https://generativelanguage.googleapis.com/v1beta/{MODEL_ID}:generateContent?key={GEMINI_API_KEY}"
    context = "\n".join([f"ID {i}: {t['title']}" for i, t in enumerate(titles_list)])
    prompt = f"ä½ æ˜¯æ“ç›˜æ‰‹ã€‚ä»ä»¥ä¸‹æ ‡é¢˜ä¸­é€‰å‡ºå½±å“æ˜æ—¥è‚¡å¸‚çš„ {target_count} æ¡ï¼Œåªè¿”å› ID åˆ—è¡¨ [1, 2, 3]ï¼š\n{context}"

    try:
        res = requests.post(url, json={"contents": [{"parts": [{"text": prompt}]}]}, timeout=300)
        if res.status_code != 200:
            print(f"âŒ Gemini åˆç­›è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {res.status_code}")
            sys.exit(1)

        raw_text = res.json()['candidates'][0]['content']['parts'][0]['text']
        ids = [int(i) for i in re.findall(r'\d+', raw_text)]
        return [titles_list[i] for i in ids if i < len(titles_list)][:target_count]
    except Exception as e:
        print(f"âŒ åˆç­›å¼‚å¸¸: {e}")
        sys.exit(1)

# 3. çˆ¬æ­£æ–‡ï¼šæ”¯æŒé•¿æ–‡æœ¬æŠ“å–
def fetch_content(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')

        # æ™ºèƒ½æŸ¥æ‰¾æœ‰æ•ˆæ®µè½ï¼ˆè¿‡æ»¤JavaScriptã€ç™»å½•ç­‰æ— å…³å†…å®¹ï¼‰
        all_ps = soup.find_all('p')
        valid_paragraphs = []

        for p in all_ps:
            text = p.get_text().strip()
            # è¿‡æ»¤æ¡ä»¶ï¼šé•¿åº¦>50ï¼Œä¸åŒ…å«å¸¸è§çš„æ— å…³è¯
            if (len(text) > 50 and
                'JavaScript' not in text and
                'ãƒ­ã‚°ã‚¤ãƒ³' not in text and
                'ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª' not in text and
                'æ©Ÿèƒ½ã‚’åˆ©ç”¨' not in text):
                valid_paragraphs.append(text)

        if valid_paragraphs:
            full_text = "\n".join(valid_paragraphs)
            return full_text[:100000]

        # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•ç‰¹å®šé€‰æ‹©å™¨
        content_selectors = [
            'div[class*="article"] p',
            'div[class*="content"] p',
            'div[class*="body"] p',
            'article p'
        ]

        for selector in content_selectors:
            ps = soup.select(selector)
            if ps and len(ps) > 2:
                paragraphs = [p.get_text().strip() for p in ps if len(p.get_text().strip()) > 50]
                if paragraphs:
                    return "\n".join(paragraphs)[:100000]

        return ""
    except Exception as e:
        print(f"  âš ï¸  æŠ“å–å¤±è´¥: {e}")
        return ""

# 4. Qwen æ‘˜è¦
def qwen_summarize(title, content, max_retries=3):
    headers = {"Authorization": f"Bearer {QWEN_API_KEY}"}
    # åˆ©ç”¨ Qwen3-8B çš„ 128K ä¸Šä¸‹æ–‡èƒ½åŠ›è¿›è¡Œå…¨æ–‡æ‘˜è¦
    payload = {
        "model": "Qwen/Qwen3-8B",
        "messages": [{"role": "user", "content": f"è¯·ä¸ºä»¥ä¸‹æ–°é—»å†™ä¸“ä¸šé‡‘èæ‘˜è¦ï¼ˆ80å­—å†…ï¼‰ï¼š\næ ‡é¢˜ï¼š{title}\næ­£æ–‡ï¼š{content}"}]
    }

    for attempt in range(max_retries):
        try:
            res = requests.post("https://api.siliconflow.cn/v1/chat/completions",
                              json=payload, headers=headers, timeout=300)
            if res.status_code == 200:
                return res.json()['choices'][0]['message']['content']
            else:
                print(f"  âš ï¸  Qwen API è¿”å›é”™è¯¯: {res.status_code}, é‡è¯• {attempt+1}/{max_retries}")
                time.sleep(2)
        except Exception as e:
            print(f"  âš ï¸  Qwen API è°ƒç”¨å¼‚å¸¸: {e}, é‡è¯• {attempt+1}/{max_retries}")
            time.sleep(2)

    return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {title}"

# 5. Gemini ç»ˆæç ”åˆ¤
def gemini_stage2_rank(summaries, target_date, max_retries=3):
    print("ğŸ† Gemini ç»ˆæç ”åˆ¤...")
    # åˆ©ç”¨ Gemini 2.5 Flash çš„è¶…å¤§ä¸Šä¸‹æ–‡å®¹é‡è¿›è¡Œå…¨é‡åˆ†æ
    prompt = f"""ä»¥ä¸‹æ˜¯å…¨é‡è´¢ç»æ–°é—»æ±‡æ€»ï¼š

{json.dumps(summaries, ensure_ascii=False)}

æˆ‘æ­£åœ¨è¿›è¡Œæ—¥è‚¡å›æµ‹ï¼Œä¸Šé¢æ˜¯ç»™ä½ çš„å…¨é‡è´¢ç»æ–°é—»æ±‡æ€»ã€‚è¯·åœ¨ä¸å‚è€ƒæœªæ¥ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

æ ¸å¿ƒçŸ›ç›¾è¯†åˆ«ï¼šæ‰¾å‡ºå½“æ—¥æ–°é—»ä¸­ï¼Œå¯¹æ—¥æœ¬è‚¡å¸‚å½±å“æœ€å¤§çš„ 3 ä¸ªå®è§‚é€»è¾‘ï¼ˆä¾‹å¦‚ï¼šæ±‡ç‡ã€åˆ©ç‡ã€æˆ–ç¾è‚¡æŸæ¿å—çš„æ˜ å°„ï¼‰ã€‚

ä¸ªè‚¡ç‹™å‡»ï¼šåŸºäºä»¥ä¸Šé€»è¾‘ï¼Œæ¨å¯¼ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆ{target_date}ï¼‰æœ€å¯èƒ½å—ç›Šçš„æ¨èæ—¥æœ¬ä¸ªè‚¡è‚¡ç¥¨ã€‚

**é‡è¦è¦æ±‚**ï¼š
1. å¿…é¡»æ˜¯çœŸå®å­˜åœ¨çš„ã€åœ¨ä¸œäº¬è¯åˆ¸äº¤æ˜“æ‰€ä¸Šå¸‚çš„å¤§å‹è‚¡ç¥¨ï¼ˆå¦‚ä¸°ç”°7203.Tã€ç´¢å°¼6758.Tã€è½¯é“¶9984.Tã€å¿«é€Ÿé›¶å”®9983.Tç­‰ï¼‰
2. ç»™å‡ºå‡†ç¡®çš„Yahoo Financeè‚¡ç¥¨ä»£ç æ ¼å¼ï¼ˆ4ä½æ•°å­—.Tï¼‰
3. ç»™å‡ºå…·ä½“æ¨å¯¼é€»è¾‘

æ¶¨è·Œé¢„æµ‹ï¼šè¯·é¢„æµ‹å¯¹åº”ä¸ªè‚¡åœ¨ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥çš„è¡¨ç°ï¼Œ**å¿…é¡»æ˜ç¡®è¯´æ˜æ˜¯"çœ‹æ¶¨"è¿˜æ˜¯"çœ‹è·Œ"**ã€‚

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰ç…§æ­¤æ ¼å¼ï¼‰ï¼š

å®è§‚é€»è¾‘ï¼š ...

æ¿å—é¢„æµ‹ï¼š æ¿å— Açœ‹å¤šï¼Œç†ç”±...

æ ¸å¿ƒä¸ªè‚¡ï¼š [è‚¡ç¥¨ä»£ç ï¼Œå¦‚7203.T]
è‚¡ç¥¨åç§°ï¼š [å…¬å¸åç§°]
é¢„æµ‹æ–¹å‘ï¼š çœ‹æ¶¨/çœ‹è·Œ
ç†ç”±ï¼š ...

é£é™©æç¤ºï¼š ..."""

    url = f"https://generativelanguage.googleapis.com/v1beta/{MODEL_ID}:generateContent?key={GEMINI_API_KEY}"

    for attempt in range(max_retries):
        try:
            # å¢åŠ timeoutåˆ°300ç§’ï¼ˆ5åˆ†é’Ÿï¼‰ï¼Œè¶³å¤Ÿå¤„ç†240æ¡æ–°é—»
            res = requests.post(url, json={"contents": [{"parts": [{"text": prompt}]}]}, timeout=300)
            if res.status_code == 200:
                return res.json()['candidates'][0]['content']['parts'][0]['text']
            else:
                print(f"  âš ï¸  Gemini API è¿”å›é”™è¯¯: {res.status_code}, é‡è¯• {attempt+1}/{max_retries}")
                time.sleep(3)
        except Exception as e:
            print(f"  âš ï¸  Gemini API è°ƒç”¨å¼‚å¸¸: {e}, é‡è¯• {attempt+1}/{max_retries}")
            time.sleep(3)

    print("âŒ Gemini ç»ˆæç ”åˆ¤å¤±è´¥")
    sys.exit(1)

# 6. æå–é¢„æµ‹ä¿¡æ¯ï¼ˆæ”¯æŒå¤šè‚¡ç¥¨ï¼‰
def extract_prediction(report_text):
    """
    ä»æŠ¥å‘Šä¸­æå–è‚¡ç¥¨ä»£ç å’Œé¢„æµ‹æ–¹å‘
    æ”¯æŒå•ä¸ªæˆ–å¤šä¸ªè‚¡ç¥¨é¢„æµ‹
    è¿”å›æ ¼å¼:
        å•ä¸ªè‚¡ç¥¨: {"stock_code": "8035.T", "direction": "çœ‹æ¶¨"}
        å¤šä¸ªè‚¡ç¥¨: [{"stock_code": "8035.T", "direction": "çœ‹æ¶¨"}, {...}]
    """

    # æŸ¥æ‰¾æ‰€æœ‰è‚¡ç¥¨ä»£ç å’Œå¯¹åº”çš„é¢„æµ‹
    predictions = []

    # æ–¹æ³•1ï¼šæŸ¥æ‰¾"æ ¸å¿ƒä¸ªè‚¡"æˆ–"æ¨èè‚¡ç¥¨"éƒ¨åˆ†
    sections = re.split(r'(æ ¸å¿ƒä¸ªè‚¡[ï¼š:]|æ ¸å¿ƒå€‹è‚¡[ï¼š:]|æ¨èè‚¡ç¥¨[ï¼š:]|æ¨è–¦è‚¡ç¥¨[ï¼š:])', report_text)

    for i, section in enumerate(sections):
        if i == 0:
            continue

        # è·å–è¿™ä¸ªsectionçš„å†…å®¹ï¼ˆä¸‹ä¸€ä¸ªå…ƒç´ ï¼‰
        if i + 1 < len(sections):
            content = sections[i + 1]

            # åœ¨è¿™ä¸ªcontentä¸­æŸ¥æ‰¾è‚¡ç¥¨ä»£ç 
            stock_codes = re.findall(r'(\d{4}\.T)', content[:500])  # åªçœ‹å‰500å­—ç¬¦

            # æŸ¥æ‰¾é¢„æµ‹æ–¹å‘ï¼ˆå…¼å®¹ç®€ç¹ä½“ï¼‰
            direction_matches = re.findall(r'(çœ‹æ¶¨|çœ‹è·Œ|çœ‹æ¼²)', content[:500])

            if stock_codes and direction_matches:
                # å¯èƒ½æœ‰å¤šä¸ªè‚¡ç¥¨
                for j, code in enumerate(stock_codes[:3]):  # æœ€å¤šå–3ä¸ª
                    direction = direction_matches[j] if j < len(direction_matches) else direction_matches[0]
                    predictions.append({
                        "stock_code": code,
                        "direction": direction.replace('æ¼²', 'æ¶¨')
                    })

    # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨å…¨å±€æœç´¢
    if not predictions:
        # æŸ¥æ‰¾æ‰€æœ‰"é¢„æµ‹æ–¹å‘"æˆ–"é¢„æµ‹"æ¨¡å¼
        pattern = r'(\d{4}\.T).*?(çœ‹æ¶¨|çœ‹è·Œ|çœ‹æ¼²)'
        matches = re.findall(pattern, report_text, re.DOTALL)

        for match in matches[:3]:  # æœ€å¤šå–3ä¸ª
            predictions.append({
                "stock_code": match[0],
                "direction": match[1].replace('æ¼²', 'æ¶¨')
            })

    # å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
    seen = set()
    unique_predictions = []
    for p in predictions:
        key = p['stock_code']
        if key not in seen:
            seen.add(key)
            unique_predictions.append(p)

    # è¿”å›æ ¼å¼ï¼šå•ä¸ªè¿”å›dictï¼Œå¤šä¸ªè¿”å›list
    if len(unique_predictions) == 0:
        return None
    elif len(unique_predictions) == 1:
        return unique_predictions[0]
    else:
        return unique_predictions

# 7. ä¿å­˜æ ‡å‡†åŒ–é¢„æµ‹æ•°æ®
def save_prediction(date_str, target_date, report, prediction, news_count, is_weekend_data=False):
    """
    ä¿å­˜é¢„æµ‹æ•°æ®ï¼Œæ ¼å¼åŒ–ä¾›å›æµ‹ä½¿ç”¨
    å¦‚æœå½“å¤©å·²æœ‰é¢„æµ‹ï¼Œä¼šè‡ªåŠ¨è¦†ç›–ï¼ˆå¤‡ä»½æ—§ç‰ˆæœ¬ï¼‰
    """
    prediction_file = f"./predictions/prediction_{date_str}.json"
    os.makedirs("./predictions", exist_ok=True)

    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆå¤‡ä»½
    if os.path.exists(prediction_file):
        backup_dir = "./predictions/backup"
        os.makedirs(backup_dir, exist_ok=True)

        # å¤‡ä»½æ–‡ä»¶ååŒ…å«æ—¶é—´æˆ³
        backup_time = datetime.now().strftime('%H%M%S')
        backup_file = f"{backup_dir}/prediction_{date_str}_backup_{backup_time}.json"

        import shutil
        shutil.copy2(prediction_file, backup_file)
        print(f"âš ï¸  æ£€æµ‹åˆ°å·²æœ‰é¢„æµ‹ï¼Œå·²å¤‡ä»½åˆ°: {backup_file}")

    # ä¿å­˜æ–°é¢„æµ‹ï¼ˆè¦†ç›–æ—§çš„ï¼‰
    data = {
        "date": date_str,
        "target_date": target_date,
        "is_weekend": is_weekend_data,
        "news_count": news_count,
        "prediction": prediction,
        "full_report": report,
        "timestamp": datetime.now().isoformat(),
        "version": "latest"  # æ ‡è®°ä¸ºæœ€æ–°ç‰ˆæœ¬
    }

    with open(prediction_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"âœ… é¢„æµ‹æ•°æ®å·²ä¿å­˜: {prediction_file}")
    print(f"   æ—¶é—´æˆ³: {data['timestamp']}")
    print(f"   ç‰ˆæœ¬: æœ€æ–° (æ—§ç‰ˆæœ¬å·²å¤‡ä»½)")

# 8. å‘¨æœ«æ¨¡å¼ï¼šç´¯ç§¯æ–°é—»
def handle_weekend_mode():
    """å‘¨æœ«æ¨¡å¼ï¼šç´¯ç§¯å‘¨äº”/å‘¨å…­/å‘¨æ—¥çš„æ–°é—»"""
    cache_file = get_weekend_cache_file()

    # è¯»å–ç°æœ‰ç¼“å­˜
    if os.path.exists(cache_file):
        with open(cache_file, "r", encoding="utf-8") as f:
            cached_data = json.load(f)
    else:
        cached_data = {"titles": [], "dates": []}

    # æŠ“å–ä»Šå¤©çš„80æ¡æ–°é—»
    today_titles = fetch_80_titles()
    today_str = datetime.now().strftime('%Y-%m-%d')

    # è¿½åŠ åˆ°ç¼“å­˜
    if today_str not in cached_data["dates"]:
        cached_data["titles"].extend(today_titles)
        cached_data["dates"].append(today_str)

        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(cached_data, f, ensure_ascii=False, indent=2)

        print(f"âœ… å‘¨æœ«æ¨¡å¼ï¼šå·²ç´¯ç§¯ {len(today_titles)} æ¡æ–°é—»ï¼ˆæ€»è®¡ {len(cached_data['titles'])} æ¡ï¼‰")

    # æ£€æŸ¥æ˜¯å¦åˆ°äº†å‘¨æ—¥æ™šä¸Šæˆ–å‘¨ä¸€å‡Œæ™¨ï¼Œè¯¥å¤„ç†äº†
    weekday = datetime.now().weekday()
    hour = datetime.now().hour

    # å‘¨æ—¥æ™šä¸Š20ç‚¹å æˆ– å‘¨ä¸€å‡Œæ™¨
    should_process = (weekday == 6 and hour >= 20) or (weekday == 0 and hour < 3)

    if should_process and len(cached_data["titles"]) > 0:
        print(f"ğŸ¯ å‘¨æœ«æ¨¡å¼ï¼šå¼€å§‹å¤„ç†ç´¯ç§¯çš„ {len(cached_data['titles'])} æ¡æ–°é—»...")
        return cached_data["titles"], True, cached_data["dates"][0]
    else:
        print(f"â³ å‘¨æœ«æ¨¡å¼ï¼šç­‰å¾…æ›´å¤šæ•°æ®... (å½“å‰ {len(cached_data['titles'])} æ¡)")
        return None, False, None

# --- æ‰§è¡Œä¸»ç¨‹åº ---
if __name__ == "__main__":
    save_dir = get_save_dir()
    today_str = datetime.now().strftime('%Y-%m-%d')

    # åˆ¤æ–­æ˜¯å¦æ˜¯å‘¨æœ«æ¨¡å¼
    if is_weekend():
        print("ğŸ“… æ£€æµ‹åˆ°å‘¨æœ«æ—¥æœŸï¼Œå¯ç”¨å‘¨æœ«æ¨¡å¼...")
        all_titles, should_process, base_date = handle_weekend_mode()

        if not should_process:
            print("âœ… ä»Šæ—¥æ–°é—»å·²ç¼“å­˜ï¼Œç­‰å¾…å‘¨æœ«ç»“æŸåç»Ÿä¸€å¤„ç†ã€‚")
            sys.exit(0)

        # å‘¨æœ«æ¨¡å¼ï¼šä»240æ¡ä¸­ç­›é€‰20æ¡
        print(f"âœ… æŠ“å–åˆ° {len(all_titles)} æ¡å‘¨æœ«ç´¯ç§¯æ ‡é¢˜ã€‚")
        top_20 = gemini_stage1_filter(all_titles, target_count=20)
        is_weekend_data = True
        date_for_save = base_date  # ä½¿ç”¨å‘¨äº”çš„æ—¥æœŸä½œä¸ºæ ‡è¯†
    else:
        # å·¥ä½œæ—¥æ¨¡å¼
        print("ğŸ“… å·¥ä½œæ—¥æ¨¡å¼...")
        all_titles = fetch_80_titles()
        print(f"âœ… æŠ“å–åˆ° {len(all_titles)} æ¡æ ‡é¢˜ã€‚")
        top_20 = gemini_stage1_filter(all_titles, target_count=20)
        is_weekend_data = False
        date_for_save = today_str

    print(f"âœ… åˆç­› {len(top_20)} æ¡æ½œåŠ›æ–°é—»å®Œæˆã€‚")

    # 3 & 4. çˆ¬å…¨æ–‡å¹¶ç”± Qwen æ€»ç»“
    summaries = []
    for i, item in enumerate(top_20):
        print(f"[{i+1}/{len(top_20)}] æ­£åœ¨æ·±åº¦è§£ææ­£æ–‡å¹¶ç”Ÿæˆæ‘˜è¦: {item['title'][:15]}...")
        raw_text = fetch_content(item['url'])
        if raw_text:
            summary = qwen_summarize(item['title'], raw_text)
            summaries.append({"title": item['title'], "summary": summary})
            time.sleep(1.5)
        else:
            print(f"  âš ï¸  æœªèƒ½è·å–æ­£æ–‡å†…å®¹ï¼Œè·³è¿‡")

    print(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(summaries)} æ¡æ–°é—»æ‘˜è¦")

    # 5. æœ€ç»ˆç ”åˆ¤
    if summaries:
        print(f"å¼€å§‹ç”Ÿæˆæœ€ç»ˆç ”åˆ¤æŠ¥å‘Š...")
        target_date = get_next_trading_day()
        report = gemini_stage2_rank(summaries, target_date)

        # æå–é¢„æµ‹ä¿¡æ¯
        prediction = extract_prediction(report)

        if prediction:
            # åˆ¤æ–­æ˜¯å•ä¸ªè¿˜æ˜¯å¤šä¸ªè‚¡ç¥¨
            if isinstance(prediction, list):
                print(f"\nğŸ¯ é¢„æµ‹ç»“æœï¼ˆ{len(prediction)}åªè‚¡ç¥¨ï¼‰:")
                for i, p in enumerate(prediction, 1):
                    print(f"  {i}. {p['stock_code']} - {p['direction']}")
            else:
                print(f"\nğŸ¯ é¢„æµ‹ç»“æœ: {prediction['stock_code']} - {prediction['direction']}")
        else:
            print(f"\nâš ï¸  æœªèƒ½ä»æŠ¥å‘Šä¸­æå–æ˜ç¡®çš„é¢„æµ‹ä¿¡æ¯")

        # ä¿å­˜ä¼ ç»ŸæŠ¥å‘Š
        report_path = f"{save_dir}/final_report.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report)

        # ä¿å­˜æ ‡å‡†åŒ–é¢„æµ‹æ•°æ®ä¾›å›æµ‹ä½¿ç”¨
        save_prediction(
            date_str=date_for_save,
            target_date=target_date,
            report=report,
            prediction=prediction,
            news_count=len(summaries),
            is_weekend_data=is_weekend_data
        )

        print(f"\nğŸ”¥ å…¨æµç¨‹ç»“æŸï¼æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        print("-" * 30)
        print(report[:500] + "...")

        # å¦‚æœæ˜¯å‘¨æœ«æ¨¡å¼ï¼Œæ¸…ç†ç¼“å­˜
        if is_weekend_data:
            cache_file = get_weekend_cache_file()
            if os.path.exists(cache_file):
                os.remove(cache_file)
                print("âœ… å‘¨æœ«ç¼“å­˜å·²æ¸…ç†")
