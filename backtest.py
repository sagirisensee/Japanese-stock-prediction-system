import json
import os
from datetime import datetime, timedelta
import yfinance as yf
from pathlib import Path

# åŠ è½½é…ç½®
try:
    from config import Config
    KEEP_DAYS = Config.PREDICTION_RETENTION_DAYS if hasattr(Config, 'PREDICTION_RETENTION_DAYS') else 30
except:
    KEEP_DAYS = 30

# é…ç½®
PREDICTIONS_DIR = "./predictions"
CUMULATIVE_STATS_FILE = "./backtest_cumulative_stats.json"  # ç´¯è®¡ç»Ÿè®¡æ–‡ä»¶

def load_cumulative_stats():
    """åŠ è½½ç´¯è®¡ç»Ÿè®¡æ•°æ®"""
    if os.path.exists(CUMULATIVE_STATS_FILE):
        try:
            with open(CUMULATIVE_STATS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass

    # é»˜è®¤åˆå§‹å€¼
    return {
        "total_predictions": 0,
        "correct_predictions": 0,
        "total_return": 0.0,
        "processed_dates": [],  # å·²å¤„ç†è¿‡çš„æ—¥æœŸåˆ—è¡¨
        "last_updated": None,
        "history": []  # ä¿ç•™æœ€è¿‘çš„è¯¦ç»†è®°å½•
    }

def save_cumulative_stats(stats):
    """ä¿å­˜ç´¯è®¡ç»Ÿè®¡æ•°æ®"""
    with open(CUMULATIVE_STATS_FILE, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    print(f"âœ… ç´¯è®¡ç»Ÿè®¡å·²æ›´æ–°: {CUMULATIVE_STATS_FILE}")

def get_stock_performance(stock_code, target_date):
    """
    è·å–è‚¡ç¥¨åœ¨ç›®æ ‡æ—¥æœŸçš„æ¶¨è·Œæƒ…å†µ
    è¿”å›: (æ¶¨è·Œå¹…ç™¾åˆ†æ¯”, æ˜¯å¦æˆåŠŸè·å–)
    """
    try:
        target = datetime.strptime(target_date, '%Y-%m-%d')
        start_date = (target - timedelta(days=5)).strftime('%Y-%m-%d')
        end_date = (target + timedelta(days=2)).strftime('%Y-%m-%d')

        ticker = yf.Ticker(stock_code)
        hist = ticker.history(start=start_date, end=end_date)

        if len(hist) < 2:
            return None, False

        target_str = target.strftime('%Y-%m-%d')

        if target_str in hist.index.strftime('%Y-%m-%d'):
            idx = list(hist.index.strftime('%Y-%m-%d')).index(target_str)
            if idx > 0:
                prev_close = hist.iloc[idx - 1]['Close']
                target_close = hist.iloc[idx]['Close']
                change_pct = ((target_close - prev_close) / prev_close) * 100
                return round(change_pct, 2), True

        available_dates = hist.index.strftime('%Y-%m-%d').tolist()
        for date_str in available_dates:
            if date_str >= target_str:
                idx = available_dates.index(date_str)
                if idx > 0:
                    prev_close = hist.iloc[idx - 1]['Close']
                    target_close = hist.iloc[idx]['Close']
                    change_pct = ((target_close - prev_close) / prev_close) * 100
                    return round(change_pct, 2), True

        return None, False

    except Exception as e:
        print(f"  âŒ è·å– {stock_code} æ•°æ®å¤±è´¥: {e}")
        return None, False

def evaluate_prediction(prediction, actual_change):
    """è¯„ä¼°é¢„æµ‹æ˜¯å¦æ­£ç¡®"""
    if actual_change is None:
        return None, None

    if prediction == "çœ‹æ¶¨":
        is_correct = actual_change > 0
        return_rate = actual_change
    elif prediction == "çœ‹è·Œ":
        is_correct = actual_change < 0
        return_rate = -actual_change
    else:
        return None, None

    return is_correct, return_rate

def clean_old_files():
    """æ¸…ç†æ—§çš„é¢„æµ‹å’ŒæŠ¥å‘Šæ–‡ä»¶ï¼Œåªä¿ç•™æœ€è¿‘ KEEP_DAYS å¤©çš„"""
    cutoff_date = datetime.now() - timedelta(days=KEEP_DAYS)

    deleted_count = 0

    # æ¸…ç†æ—§çš„é¢„æµ‹æ–‡ä»¶
    if os.path.exists(PREDICTIONS_DIR):
        for pred_file in Path(PREDICTIONS_DIR).glob("prediction_*.json"):
            try:
                # ä»æ–‡ä»¶åæå–æ—¥æœŸ prediction_2026-02-05.json
                date_str = pred_file.stem.replace("prediction_", "")
                file_date = datetime.strptime(date_str, '%Y-%m-%d')

                if file_date < cutoff_date:
                    pred_file.unlink()
                    deleted_count += 1
                    print(f"  ğŸ—‘ï¸  åˆ é™¤æ—§é¢„æµ‹: {pred_file.name}")
            except:
                pass

    # æ¸…ç†æ—§çš„æŠ¥å‘Šç›®å½•
    for report_dir in Path(".").glob("report_*"):
        if report_dir.is_dir():
            try:
                # ä»ç›®å½•åæå–æ—¥æœŸ report_20260205
                date_str = report_dir.name.replace("report_", "")
                file_date = datetime.strptime(date_str, '%Y%m%d')

                if file_date < cutoff_date:
                    import shutil
                    shutil.rmtree(report_dir)
                    deleted_count += 1
                    print(f"  ğŸ—‘ï¸  åˆ é™¤æ—§æŠ¥å‘Š: {report_dir.name}/")
            except:
                pass

    # æ¸…ç†æ—§çš„å›æµ‹ç»“æœæ–‡ä»¶
    for backtest_file in Path(".").glob("backtest_result_*.json"):
        try:
            # backtest_result_20260205_120000.json
            parts = backtest_file.stem.replace("backtest_result_", "").split("_")
            if len(parts) >= 1:
                date_str = parts[0]
                file_date = datetime.strptime(date_str, '%Y%m%d')

                if file_date < cutoff_date:
                    backtest_file.unlink()
                    deleted_count += 1
                    print(f"  ğŸ—‘ï¸  åˆ é™¤æ—§å›æµ‹: {backtest_file.name}")
        except:
            pass

    if deleted_count > 0:
        print(f"\nâœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ—§æ–‡ä»¶")
    else:
        print(f"\nâœ… æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ—§æ–‡ä»¶")

def run_incremental_backtest():
    """è¿è¡Œå¢é‡å›æµ‹ï¼šåªå¤„ç†æ–°çš„é¢„æµ‹æ–‡ä»¶"""
    print("=" * 60)
    print("å¼€å§‹å¢é‡å›æµ‹...")
    print("=" * 60)

    if not os.path.exists(PREDICTIONS_DIR):
        print(f"âŒ é¢„æµ‹ç›®å½•ä¸å­˜åœ¨: {PREDICTIONS_DIR}")
        return

    # åŠ è½½ç´¯è®¡ç»Ÿè®¡
    cumulative = load_cumulative_stats()
    processed_dates = set(cumulative["processed_dates"])

    print(f"ğŸ“Š å½“å‰ç´¯è®¡ç»Ÿè®¡:")
    print(f"   æ€»é¢„æµ‹æ¬¡æ•°: {cumulative['total_predictions']}")
    print(f"   æ­£ç¡®æ¬¡æ•°: {cumulative['correct_predictions']}")
    if cumulative['total_predictions'] > 0:
        accuracy = (cumulative['correct_predictions'] / cumulative['total_predictions']) * 100
        print(f"   ç´¯è®¡æ­£ç¡®ç‡: {accuracy:.2f}%")
        print(f"   ç´¯è®¡æ”¶ç›Šç‡: {cumulative['total_return']:+.2f}%")
    print(f"   å·²å¤„ç†æ—¥æœŸæ•°: {len(processed_dates)}")
    print()

    # è¯»å–æ‰€æœ‰é¢„æµ‹æ–‡ä»¶
    prediction_files = sorted([
        f for f in Path(PREDICTIONS_DIR).glob("prediction_*.json")
        if "backup" not in str(f)
    ])

    if not prediction_files:
        print(f"âŒ æœªæ‰¾åˆ°é¢„æµ‹æ–‡ä»¶")
        return

    # åªå¤„ç†æœªå¤„ç†è¿‡çš„æ–‡ä»¶
    new_files = [f for f in prediction_files
                 if f.stem.replace("prediction_", "") not in processed_dates]

    if not new_files:
        print("âœ… æ²¡æœ‰æ–°çš„é¢„æµ‹éœ€è¦å›æµ‹")
        return

    print(f"ğŸ” å‘ç° {len(new_files)} ä¸ªæ–°é¢„æµ‹æ–‡ä»¶\n")

    new_results = []

    for pred_file in new_files:
        print(f"å¤„ç†æ–‡ä»¶: {pred_file.name}")

        with open(pred_file, 'r', encoding='utf-8') as f:
            pred_data = json.load(f)

        date = pred_data.get('date')
        prediction_info = pred_data.get('prediction')

        if not prediction_info:
            print(f"  âš ï¸  æœªæ‰¾åˆ°é¢„æµ‹ä¿¡æ¯ï¼Œè·³è¿‡")
            continue

        # å¤„ç†é¢„æµ‹ï¼ˆæ”¯æŒå•ä¸ªæˆ–å¤šä¸ªè‚¡ç¥¨ï¼‰
        predictions_list = [prediction_info] if isinstance(prediction_info, dict) else prediction_info

        # ä¿®æ­£ï¼šé¢„æµ‹æ–‡ä»¶é¢„æµ‹çš„æ˜¯å½“å¤©(date)çš„æ¶¨è·Œï¼Œè€Œä¸æ˜¯target_date
        print(f"  é¢„æµ‹æ—¥æœŸ: {date}")

        for idx, pred in enumerate(predictions_list, 1):
            stock_code = pred.get('stock_code')
            direction = pred.get('direction')

            if not stock_code or not direction:
                continue

            if len(predictions_list) > 1:
                print(f"  è‚¡ç¥¨ {idx}/{len(predictions_list)}: {stock_code}")

            # è·å–å®é™…è¡¨ç° - ä½¿ç”¨é¢„æµ‹æ—¥æœŸdateè€Œä¸æ˜¯target_date
            actual_change, success = get_stock_performance(stock_code, date)

            if not success:
                print(f"  âš ï¸  æ— æ³•è·å–æ•°æ®ï¼Œè·³è¿‡")
                continue

            print(f"  é¢„æµ‹: {direction}, å®é™…: {actual_change:+.2f}%", end=" ")

            # è¯„ä¼°
            is_correct, return_rate = evaluate_prediction(direction, actual_change)

            if is_correct is None:
                print("âš ï¸  æ— æ³•è¯„ä¼°")
                continue

            # æ›´æ–°ç´¯è®¡ç»Ÿè®¡
            cumulative["total_predictions"] += 1
            if is_correct:
                cumulative["correct_predictions"] += 1
                print("âœ… æ­£ç¡®", end="")
            else:
                print("âŒ é”™è¯¯", end="")

            print(f", æ”¶ç›Š: {return_rate:+.2f}%")

            cumulative["total_return"] += return_rate

            # è®°å½•è¯¦ç»†ç»“æœï¼ˆåªä¿ç•™æœ€è¿‘çš„ï¼‰
            new_results.append({
                "date": date,
                "stock_code": stock_code,
                "prediction": direction,
                "actual_change": float(actual_change),
                "is_correct": bool(is_correct),
                "return_rate": float(return_rate)
            })

        # æ ‡è®°ä¸ºå·²å¤„ç†
        processed_dates.add(date)
        print()

    # æ›´æ–°å†å²è®°å½•ï¼ˆåªä¿ç•™æœ€è¿‘çš„ï¼‰
    cumulative["history"].extend(new_results)
    cumulative["history"] = cumulative["history"][-100:]  # åªä¿ç•™æœ€è¿‘100æ¡
    cumulative["processed_dates"] = sorted(list(processed_dates))
    cumulative["last_updated"] = datetime.now().isoformat()

    # ä¿å­˜ç´¯è®¡ç»Ÿè®¡
    save_cumulative_stats(cumulative)

    # è¾“å‡ºæœ€æ–°ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("å›æµ‹ç»“æœæ±‡æ€»ï¼ˆç´¯è®¡ï¼‰")
    print("=" * 60)

    if cumulative["total_predictions"] > 0:
        accuracy = (cumulative["correct_predictions"] / cumulative["total_predictions"]) * 100
        avg_return = cumulative["total_return"] / cumulative["total_predictions"]

        print(f"ğŸ“Š æ€»é¢„æµ‹æ¬¡æ•°: {cumulative['total_predictions']}")
        print(f"âœ… æ­£ç¡®æ¬¡æ•°: {cumulative['correct_predictions']}")
        print(f"âŒ é”™è¯¯æ¬¡æ•°: {cumulative['total_predictions'] - cumulative['correct_predictions']}")
        print(f"ğŸ¯ ç´¯è®¡æ­£ç¡®ç‡: {accuracy:.2f}%")
        print(f"ğŸ’° å¹³å‡æ”¶ç›Šç‡: {avg_return:+.2f}%")
        print(f"ğŸ’° ç´¯ç§¯æ€»æ”¶ç›Š: {cumulative['total_return']:+.2f}%")
        print(f"ğŸ“… è¦†ç›–å¤©æ•°: {len(processed_dates)}")

    print("=" * 60)

    # æ˜¾ç¤ºæœ€è¿‘çš„è¯¦ç»†ç»“æœ
    if new_results:
        print("\næœ¬æ¬¡æ–°å¢ç»“æœ:")
        print("-" * 100)
        print(f"{'æ—¥æœŸ':<12} {'è‚¡ç¥¨':<10} {'é¢„æµ‹':<6} {'å®é™…æ¶¨è·Œ':<10} {'ç»“æœ':<6} {'æ”¶ç›Šç‡':<10}")
        print("-" * 100)
        for r in new_results:
            result_symbol = "âœ“" if r['is_correct'] else "âœ—"
            print(f"{r['date']:<12} {r['stock_code']:<10} {r['prediction']:<6} "
                  f"{r['actual_change']:+7.2f}%   {result_symbol:<6} {r['return_rate']:+7.2f}%")
        print("-" * 100)

if __name__ == "__main__":
    print("\nğŸ“Š æ—¥è‚¡é¢„æµ‹å›æµ‹ç³»ç»Ÿï¼ˆå¢é‡æ¨¡å¼ï¼‰\n")

    # è¿è¡Œå¢é‡å›æµ‹
    run_incremental_backtest()

    # æ¸…ç†æ—§æ–‡ä»¶
    print("\nğŸ—‘ï¸  æ£€æŸ¥æ˜¯å¦æœ‰æ—§æ–‡ä»¶éœ€è¦æ¸…ç†...")
    clean_old_files()
